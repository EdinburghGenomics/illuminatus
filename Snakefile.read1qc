#!/bin/bash
# vim: ft=python

## This workflow specifically runs the well dups scanner.
## This triggers before demultiplexing but contributes to the
## final QC report so it makes sens for it to be separate.
## For non-patterned flowcells this will just be a no-op.

# Contents >>>
#   + Embedded BASH script to bootstrap the workflow
#   + Initialisation and configuration
#   + Helper functions
#   + The rules specific to this workflow
#   + More generic rules

"""true" ### Begin shell script part
set -euo pipefail

source "`dirname $0`"/shell_helper_functions.sh
export TOOLBOX="$(find_toolbox)"

snakerun_drmaa "$0" "$@"

"exit""" ### End of shell script part

#!/usr/bin/env snakemake
import yaml
import re
import xml.etree.ElementTree as ET
from snakemake.utils import format

# See notes in Snakefile.qc regarding the toolbox.
# Here we need wd_get_cached_targets and wd_count_well_duplicates to be available.
TOOLBOX = 'env PATH="{}:$PATH"'.format(os.environ['TOOLBOX'])

def glob():
    """Regular glob() is useful but it can be improved like so.
    """
    from glob import glob
    return lambda p: sorted( (f.rstrip('/') for f in glob(os.path.expanduser(p))) )
glob = glob()

# Caller can set this, or else there needs to be a symlink, as with Snakefile.demux
RUNDIR = os.path.realpath(config.get('rundir', './seqdata'))

def get_wd_settings(rundir):
    """Settings for well duplicates scanning.
    """
    wd_settings = dict( TARGETS_TO_SAMPLE = 2500,
                        READ_LENGTH = 50,
                        START_POS = 20,
                        LEVELS_TO_SCAN = 5,
                        REPORT_VERBOSE = True )

    # We need to examine the run info. OK to do this on every cluster job and
    # fail immediately if missing.
    wd_settings.update(dict( LANES_TO_SAMPLE = [],
                             END_POS = 0,
                             TILE_MATCH = dict() ))
    try:
        run_info_root = ET.parse(rundir + "/RunInfo.xml").getroot()
    except FileNotFoundError:
        return wd_settings

    wd_settings['LAST_LANE'] = '0'
    wd_settings['LAST_TILE'] = wd_settings['FIRST_TILE'] = '0000'
    try:
        wd_settings['LAST_LANE'], wd_settings['LAST_TILE'] = \
            max(te.text for te in run_info_root.findall(".//Tiles/Tile")).split('_')
        _, wd_settings['FIRST_TILE'] = \
            min(te.text for te in run_info_root.findall(".//Tiles/Tile")).split('_')
    except ValueError:
        # If there are no tiles we can reasonably assume we're on a non-patterned
        # Flowell (MiSeq or 2500). In this case, do nothing., which we can achieve by
        # inserting a no-op rule if LAST_LANE == 0 - see wd_main.
        pass

    # Lanes to sample is now variable since the arrival of Novaseq, so get it from
    # RunInfo.xml...
    wd_settings['LANES_TO_SAMPLE'] = list(range(1, int(wd_settings['LAST_LANE']) + 1))

    if wd_settings['LANES_TO_SAMPLE']:
        # Find the first read that has >READ_LENGTH cycles. Normally read1 will do.
        # If not using read1, always set START_POS to the first cycle of that read.
        # Also scanning later reads will fail when triggered early if there are missing BCL
        # files but that's OK as the pipeline will simply retry once the run is complete.
        all_read_lens = [ int(r.get('NumCycles')) for r in
                          sorted( run_info_root.findall("Run/Reads/Read"),
                                  key = lambda r: int(r.get("Number")) ) ]
        if max(all_read_lens) > wd_settings['READ_LENGTH']:
            read_to_sample = [ n for n, l in enumerate(all_read_lens)
                               if l > wd_settings['READ_LENGTH'] ][0]

            # Is it OK to start at START_POS or do we go from 0?
            if read_to_sample == 0 and \
               all_read_lens[read_to_sample] > (wd_settings['READ_LENGTH'] + wd_settings['START_POS']):
                start_pos = wd_settings['START_POS']
            else:
                start_pos = 0

            # Now we have the relative START_POS, calculate the absolute START_POS and
            # overwrite the initial setting
            wd_settings['START_POS'] = start_pos + sum( all_read_lens[:read_to_sample] )
        else:
            # We can't process this run
            print("Cannot process run with short read lengths {}.".format(all_read_lens), file=sys.stderr)
            wd_settings['LANES_TO_SAMPLE'] = []

        wd_settings['END_POS'] = wd_settings['READ_LENGTH'] + wd_settings['START_POS']

    # If there are 4 swaths per side (NovaSeq) then only do the even tiles
    if wd_settings['LAST_TILE'] >= '2400':
        wd_settings['TILE_MATCH'] = dict( T='1..[02468]', B='2..[02468]' )
    else:
        wd_settings['TILE_MATCH'] = dict( T='1...', B='2...' )

    # On the new NovaSeq SP flowcells they seem to be disabling one surface, so:
    if wd_settings['LAST_TILE'][0] == '1':
        del wd_settings['TILE_MATCH']['B']
    elif wd_settings['FIRST_TILE'][0] == '2':
        del wd_settings['TILE_MATCH']['T']

    # Unfortunately there's more - for slimmed-down runs we get a line in
    # pipeline_settings.ini like this, indicating that most tiles are missing:
    #  --tiles: s_[$LANE]_1101
    # I don't want to just ignore tiles with missing BCL files as this is right to
    # trigger a genuine validation failure.
    # Rather than re-code the logic for what goes in pipeline_settings.ini I'm just going
    # to scan the file and interpret the line.
    # This is dodgy in the general case but OK for test runs!
    try:
        with open(rundir + "/pipeline_settings.ini") as fh:
            for l in fh:
                mo = re.match(r'''\s*--tiles:.*]_(\d{4})['"]?$''', l)
                if mo:
                    tile = mo.group(1)
                    wd_settings['TILE_MATCH'] = dict( T=tile ) if tile[0] == '1' else dict( B=tile )
    except Exception:
        #no matter
        pass

    return wd_settings

# Set the returned dict items as global variables
globals().update(get_wd_settings(RUNDIR))

# === Driver rules ===
localrules: wd_main, bc_main

if not LANES_TO_SAMPLE:
    rule wd_main:
        # No-op rule
        shell: "# No lanes to be sampled. Assuming this is a MiSeq or 2500 run, or read1 is too short."
else:
    rule wd_main:
        input:
            summary = format('QC/welldups/{TARGETS_TO_SAMPLE}summary.yml')

rule bc_main:
    # TODO - add logic for early demux check
    input: []

# === Rules to invoke the duplicate counter ===
localrules: make_wd_summary

rule make_wd_summary:
    output: 'QC/welldups/{targets}summary.yml'
    input: expand( "QC/welldups/{{targets}}targets_lane{lane}{side}.txt", lane=LANES_TO_SAMPLE, side=TILE_MATCH.keys() )
    run:
        from collections import deque
        from time import sleep
        sleep(2) # Avoids annoying re-running due to clock skew

        # TODO - I really should make a unit test for this!
        # Result will be of the form { lane: { side: { metric: 12.34 } } }
        res = dict()

        for f in input:
            # Since files end in 2T.txt or whatever, populate res
            lane = res.setdefault(f[-6], dict())
            side = lane.setdefault(f[-5], dict())

            #Look at the last 3 lines of the file
            with open(f) as fh:
                for k, v in [ l.rstrip('\n%').split(':') for l in deque(fh, 3) ]:
                    if "Overall" in k:
                        side["raw"] = float(v)
                    elif "v1" in k:
                        side["v1"] = float(v)
                    elif "v2" in k:
                        side["v2"] = float(v)
        # Now average things up.
        for lane in res.values():
            lane['mean'] = { metric: sum(side[metric] for side in lane.values()) / len(lane)
                             for metric in list(lane.values())[0].keys() }

        # And save it out
        with open(output[0], 'w') as yfh:
            yaml.safe_dump(res, yfh)

rule prep_wd_indices:
    output: "QC/welldups/{targets}clusters.list"
    input:
        slocs = format("{RUNDIR}/Data/Intensities/s.locs")
    shell:
        "{TOOLBOX} wd_get_cached_targets {input.slocs} {TARGETS_TO_SAMPLE} {output}"

rule count_well_dupl:
    output: "QC/welldups/{targets}targets_lane{lane}{side,.}.txt"
    input:
        targfile = "QC/welldups/{targets}clusters.list"
    params:
        verbose = '' if REPORT_VERBOSE else '-S',
        tiles   = lambda wc: TILE_MATCH[wc.side]
    shell:
        """{TOOLBOX} wd_count_well_duplicates -f {input.targfile} -n {wildcards.targets} -s {LAST_TILE} \
           -r {RUNDIR} -i {wildcards.lane} -l {LEVELS_TO_SCAN} --cycles {START_POS}-{END_POS} \
           -t {params.tiles} {params.verbose} > {output}"""

# === Rules to pre-demultiplex one tile to check the barcode balance ===

# TODO
